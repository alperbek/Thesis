\section{Introduction}

Convolutional Neural Networks have achieved state-of-the-art results in various computer vision tasks~\cite{He2016DeepRL, Lin2018FocalLF}. 
%Most of the success is lead by innovation in designing more complex and task-specific networks~\cite{He2017MaskR, Ronneberger2015UNetCN}.
Much of this success is due to innovations of a novel, task-specific network architectures~\cite{He2017MaskR, Ronneberger2015UNetCN}.
%All these networks are trained using similar optimization techniques, which are principally designed for a generic neural network which treats each weight as a single entity - \textit{a neuron}.
Despite variation in network design, the same core optimization techniques are used across tasks.
These techniques consider each individual weight as its own entity and update them independently.
%Limited innovation has been made at understanding training process categorically designed for convolutional networks where a single unit is a \textit{filter}. 
Limited progress has been made towards developing a training process specifically designed for convolutional networks, in which \textit{filters} are the fundamental unit of the network.
A filter is not a single weight parameter but a stack of spatial kernels.
%One of the implications of this is that most models are over-parameterized and a lot of the filters are redundant. 

Because models are typically over-parameterized, a trained convolutional network will contain redundant filters~\cite{Cogswell2015ReducingOI, Li2016PruningFF}. %TODO: cite over-parameterized?
%This is evident from abundant research in model compression which prunes complete filters~\cite{He2017ChannelPF, Anwar2017StructuredPO, Li2016PruningFF, Molchanov2016PruningCN, Liu2017LearningEC, Luo2017ThiNetAF} rather than just individual parameters~\cite{Han2015DeepCC}.
This is evident from the common practice of pruning filters~\cite{He2017ChannelPF, Anwar2017StructuredPO, Li2016PruningFF, Molchanov2016PruningCN, Liu2017LearningEC, Luo2017ThiNetAF}, rather than individual parameters~\cite{Han2015DeepCC}, to achieve model compression.
Most of these pruning methods are able to drop a significant number of filters with only a marginal loss in the performance of the model.
However, a model with fewer filters cannot be trained from scratch to achieve the performance of a large model that has been pruned to be roughly the same size~\cite{Li2016PruningFF, Luo2017ThiNetAF, Zhu2017ToPO}.
%The current training procedure is not able to find the same efficient filters that are obtained from the pruned models, which suggests there is a room for improvement for the training of Convolutional Neural Networks (ConvNets).
Standard training procedures tend to learn models with extraneous and prunable filters, even for architectures without any excess capacity.
This suggests that there is room for improvement in the training of Convolutional Neural Networks (ConvNets).

\begin{figure}[H]
%\hspace*{-2mm}
\center
   % \hspace{-8.0mm}
   \includegraphics[width=0.47\linewidth]{figures/repr/RePr_Train.pdf}
   %\hspace{-3.0mm}
   \includegraphics[width=0.45\linewidth]{figures/repr/RePr_Test.pdf}
   %\hspace{-8.0mm}
   %\includegraphics[width=1.05\linewidth]{figures/repr/combine_repr.pdf}
   %\vspace{-3mm}
   \caption[Performance Standard vs RePr]{Performance of a three layer ConvNet with 32 filters each over $100$ epochs using \textcolor{orange}{standard scheme} and \textcolor{blue}{our method - RePr} on CIFAR-$10$. The shaded regions denote periods when only part of the network is trained. 
   Left: Training Accuracy, Right: Test Accuracy. Annotations [A-F] are discussed in Section~\ref{sec:training}.}
   \label{fig:REPR}
   %%\vspace{1mm}
\end{figure}

%To that aim, we suggest a training scheme whereby after certain iterations of standard training, we mark some percentage of filters based on pre-determined criteria and continue training the rest of the network temporarily without the marked filters. 
To this end, we propose a training scheme in which, after some number of iterations of standard training, we select a subset of the model's filters to be temporarily dropped.
%After few epochs of training the reduced network, we reintroduce the dropped filters with new starting weights. 
After additional training of the reduced network, we reintroduce the previously dropped filters, initialized with new weights, and continue standard training.
%On every the reintroduction of the dropped filters the model's performance jumps higher than the value before the drop. 
We observe that following the reintroduction of the dropped filters, the model is able to achieve higher performance than was obtained before the drop. 
Repeated application of this process obtains models which outperform those obtained by standard training as seen in Figure ~\ref{fig:REPR} and discussed in Section~\ref{sec:training}.
%Repeating this process a couple of times with diminishing returns yields a model that has improved performance compared to standard training alone. 
We observe this improvement across various tasks and over various types of convolutional networks.
%This training technique, regardless of the filter dropping criteria, in itself leads to improved performance.
This training procedure is able to produce improved performance across a range of possible criteria for choosing which filters to drop, and further gains can be achieved by careful selection of the ranking criterion.
According to a recent hypothesis~\cite{Frankle2018TheLT}, the relative success of over-parameterized networks may largely be due to an abundance of initial sub-networks.
%Our technique allows shallow models to seek larger space of sub-networks by repeated re-initialization.
Our method aims to preserve successful sub-networks while allowing the re-initialization of less useful filters.


%Both of these techniques benefit from giving parameters a second chance at initialization.
%Recently, this has been hypothesized to be a reason for the success of large networks over small ones~\cite{Frankle2018TheLT}.
%In addition to the training scheme a major contribution of our work is in exploring an efficient metric for temporary filter dropping that will lead to a model that significantly outperforms the standard training. 
In addition to our novel training strategy, the second major contribution of our work is an exploration of metrics to guide filter dropping.
%We show that ranking criteria used for filter pruning (permanent) is not the right choice, and does not malleable to our training scheme.
Our experiments demonstrate that standard techniques for permanent filter pruning are suboptimal in our setting, and we present an alternative metric which can be efficiently computed, and which gives a significant improvement in performance.
%We propose a metric based on the \textit{inter-}filter orthogonality of convolutional layers as the most effective ranking measure for this alternative training scheme. 
We propose a metric based on the \textit{inter-}filter orthogonality within convolutional layers and show that this metric outperforms state-of-the-art filter importance ranking methods used for network pruning in the context of our training strategy.
%We explore most of the widely known and state-of-the-art filter importance ranking criteria used for pruning networks and show that our metric out-performs all of them while keeping the computation cost on the lower side.
%Several researchers have observed that ConvNets learn over-lapping representations and have redundant filters~\cite{Cogswell2015ReducingOI, Li2016PruningFF}.
We observe that even small, under-parameterized networks tend to learn redundant filters, which suggests that filter redundancy is not solely a result of over-parameterization, but is also due to ineffective training.
%We observed that this is also true for shallow networks and not just deep networks.
%Thus the redundancy is not necessarily due to over-parameterization but also to some extent due to ineffective training.
Our goal is to reduce the redundancy of the filters and increase the expressive capacity of ConvNets and we achieve this by changing the training scheme rather than the model architecture.



\section{Related Work}

%We have divided the discussion of related works into categories that broadly cover our area of research.
\subsection{Training Scheme } 
Many changes to the training paradigm have been proposed to reduce over-fitting and improve generalization.
Dropout~\cite{Wu2015TowardsDT} is widely used in training deep nets. 
By stochastically dropping the neurons it prevents co-adaption of feature detectors.
A similar effect can be achieved by dropping a subset of activations ~\cite{Wan2013RegularizationON}.
Wu \etal~\cite{Wu2015TowardsDT} extend the idea of stochastic dropping to convolutional neural networks by probabilistic pooling of convolution activations.
Yet another form of stochastic training recommends randomly dropping entire layers~\cite{Huang2016DeepNW}, forcing the model to learn similar features across various layers which prevent extreme overfitting.
In contrast, our technique encourages the model to use a linear combination of features instead of duplicating the same feature.
Han \etal~\cite{Han2016DSDDT} propose Dense-Sparse-Dense (DSD), a similar training scheme, in which they apply weight regularization mid-training to encourage the development of sparse weights, and subsequently remove the regularization to restore dense weights.
They recommend adding weight regularization to make sparse weights and removing the regularization subsequently to achieve dense network again.
DSD works on individual parameters and uses weight norm to drop the weights. 
We designed our training scheme for specially convolutional filters and we show that it is important to allow each of the model (with reduced filters) to converge before switching over. 
While DSD works at the level of individual parameters, our method is specifically designed to apply to convolutional filters.
We achieve this by allowing the sub-network to relearn any missing feature representations with existing filters.
In a limited model capacity, doing so encourages more expressivity~\cite{Raghu2017OnTE} and as our results show it leads to efficient model.



\subsection{Model Compression } 
Knowledge Distillation (KD)~\cite{Hinton2015DistillingTK} is a training scheme which uses soft logits from a larger trained model (teacher) to train a smaller model (student).
Soft logits capture hierarchical information about the object and provide a smoother loss function for optimization. This leads to easier training and better convergence for small models.
Stronger forms of KD where a model matches intermediate activations have also been proposed~\cite{Romero2014FitNetsHF}. 
By forcing the student model to match the output statistics from the teacher, it provides the directions for weight updates which are not found in the gradients.
In a surprising result, Born-Again-Network~\cite{Furlanello2018BornAN} shows that if the student model is of the same capacity as the teacher it can outperform the teacher.
A few other variants of KD have been proposed~\cite{Romero2014FitNetsHF} and all of them require training several models.
Our training scheme does not depend on an external teacher and requires less training than KD. %More importantly, our scheme is complementary to KD. 
More importantly, when combined with KD, our method gives better performance than can be achieved by either technique independently (discussed in Section~\ref{lbl:distillation}).


\subsection{Neuron ranking }
Interest in finding the least salient neurons/weights has a long history. 
LeCun~\cite{LeCun1989OptimalBD} and Hassibi\etal~\cite{Hassibi1992SecondOD} show that using the Hessian, which contains second-order derivative, identifies the weak neurons and performs better than using the magnitude of the weights.
Computing the Hessian is expensive and thus is not widely used.
Han \etal~\cite{Han2015DeepCC} show that the norm of weights is still effective ranking criteria and yields sparse models.
The sparse models do not translate to faster inference, but as a neuron ranking criterion, they are effective.
Computing the norm of the weights is easy and data-free, thus lends itself to pruning for transfer learning.
Hu \etal~\cite{Hu2016NetworkTA} explore Average Percentage of Zeros (APoZ) in the activations and use a data-driven threshold to determine the cut-off. 
It is a useful comparison metric and works well when the training data is available.
ThinNet~\cite{Luo2017ThiNetAF} explores reconstruction error between the layers, this does not lead to a generalized filter ranking criteria.
Molchanov \etal~\cite{Molchanov2016PruningCN} recommend the second term from the Taylor expansion of the loss function.%, which is a product of activation and the gradient.
Currently, it is the state-of-the-art in filter pruning and works well for transfer pruning.
Our filter ranking criteria, \textit{inter}-filter orthogonality, is yet another approach and is designed especially for the purposes of temporary pruning. 
We provide detail comparison and show results on using these metrics with our training scheme in Section~\ref{sec:metric}.

Chen \etal ~\cite{Chen2017TrainingGO} used privileged information, like segmentation maps, to encourage diversity in feature maps, and approximate group orthogonal filters.
Unfortunately, such techniques restrict themselves to domains where segmentation maps are available for the task of image classification. 
Our method does not rely on any external information or a pretrained model.

\subsection{Architecture Search } 

Neural architecture search ~\cite{Liu2017ProgressiveNA, Real2018RegularizedEF, Zoph2016NeuralAS} is where the architecture is modified during training, and multiple neural network structures are explored in search of the best architecture for a given dataset. 
Such methods do not have any benefits if the architecture is fixed ahead of time. Our scheme improves training for a given architecture by making better use of the available parameters. 
This could be used in conjunction with architecture search if there is flexibility around the final architecture or used on its own when the architecture is fixed due to certified model deployment, memory requirements, or other considerations.
Our scheme is finds better weights given a fixed architecture and thus have a lower potential than the NAS models.
Potential for finding a better generalizing architecture diminishes by restricting to a fixed final architecture but asserts itself to several applications where predetermined architecture is warranted.
This could be due to certified model deployment, memory requirements or better interpretability of simpler ConvNets.

\subsection{Feature correlation } 
A well-known shortcoming of vanilla convolutional networks is their correlated feature maps~\cite{Cogswell2015ReducingOI, Glorot2010UnderstandingTD}.
Architectures like Inception-Net~\cite{Szegedy2015GoingDW} are motivated by analyzing the correlation statistics of features across layers.
They aim to reduce the correlation between the layers by using concatenated features from various sized filters, subsequent research shows otherwise~\cite{Raghu2017SVCCASV}.
%However, correlation of singular values across inception layers show that there is a significant overlap of activations. 
More recent architectures like ResNet~\cite{He2016DeepRL} and DenseNet ~\cite{Huang2017DenselyCC} aim to implicitly reduce feature correlations by summing or concatenating activations from previous layers.
That said, these models are computationally expensive and require large memory to store previous activations.
Our aim is to induce decorrelated features without changing the architecture of the convolutional network.
This benefits all the existing implementations of ConvNet without having to change the infrastructure.
While our technique performs best with vanilla ConvNet architectures it still marginally improves the performance of modern architectures.

\section{Motivation for Orthogonal Features}
A feature for a convolutional filter is defined as the point-wise sum of the activations from individual kernels of the filter.
A feature is considered useful if it helps to improve the generalization of the model.
A model that has poor generalization usually has features that, in aggregate, capture limited directions in activation space~\cite{Morcos2017OnTI}.
%If the features are orthogonal they will be less correlated and capture more of the directions.
On the other hand, if a model's features are orthogonal to one another, they will each capture distinct directions in activation space, leading to improved generalization.
%For a trivial sized ConvNet, maximally expressive weights is computed by analyzing the correlation of features across layers and clustering them to common groups~\cite{Arora2014ProvableBF}. 
For a trivially-sized ConvNet, we can compute the maximally expressive filters by analyzing the correlation of features across layers and clustering them into groups~\cite{Arora2014ProvableBF}.
%However, this scheme for finding weights is impractical for a typical sized deep ConvNet. 
However, this scheme is computationally impractical for the deep ConvNets used in real-world applications.
%Instead of adding a regularization during standard SGD training minimizes the covariance of activations leads to slightly better models~\cite{Rodrguez2016RegularizingCW, Cogswell2015ReducingOI}.
Alternatively, a computationally feasible option is the addition of a regularization term to the loss function used in standard SGD training which encourages the minimization of the covariance of the activations, but this produces only limited improvement in model performance~\cite{Rodrguez2016RegularizingCW, Cogswell2015ReducingOI}.
%Similarly, marginal improvements have been observed by regularizing the orthogonality of weights during training~\cite{Brock2016NeuralPE, Poole2014AnalyzingNI, Xie2017NearOrthogonalityRI}.
A similar method, in which the regularization term instead encourages the orthogonality of filter weights, has also produced marginal improvements~\cite{Brock2016NeuralPE, Poole2014AnalyzingNI, Xie2017NearOrthogonalityRI, Xie2017AllYN}.
Shang \etal~\cite{Shang2016UnderstandingAI} discovered the low-level filters are duplicated with opposite phase. 
%This can be prevented by using CReLU in place of ReLU, but the later still remains the most common choice for the activation function.
Forcing filters to be orthogonal will minimize this duplication without changing the activation function.
%These techniques fall short of finding the filters that are perfectly decorrelated but are practical for everyday use.
%An another advantage of weights being orthogonal are that they lead to stable convergence of deep networks~\cite{Saxe2013ExactST}.
In addition to improvements in performance and generalization, Saxe \etal ~\cite{Saxe2013ExactST} show that the orthogonality of weights also improves the stability of network convergence during training.
The authors of ~\cite{Xie2017AllYN, Xiao2018DynamicalIA} further demonstrate the value of orthogonal weights to the efficient training of networks.
%This is also why Recurrent Neural Networks are often initialized with orthogonal weights due to their sensitivity of initial point~\cite{Vorontsov2017OnOA} even if this is no longer popular for ConvNets.
Orthogonal initialization is common practice for Recurrent Neural Networks due to their increased sensitivity to initial conditions~\cite{Vorontsov2017OnOA}, but it has somewhat fallen out of favor for ConvNets.
%Yet another advantage of orthogonal weights is that they lead to the efficient training of very deep networks~\cite{Xie2017AllYN, Xiao2018DynamicalIA}.
These factors shape our motivation for encouraging orthogonality of features in the ConvNet and form the basis of our ranking criteria.

Collecting activations over a dataset in order to compute orthogonal features is slow and inefficient. Instead, we use orthogonality of weights, to represent feature overlap.
Because features are dependent on the input data, determining their orthogonality requires computing statistics across the entire training set, and is therefore prohibitive.  
We instead compute the orthogonality of filter weights as a surrogate.
Our experiments show that encouraging weight orthogonality through a regularization term is insufficient to promote the development of features which capture the full space of the input data manifold.
Our method of dropping overlapping filters acts as an implicit regularization and leads to the better orthogonality of filters without hampering model convergence.


We use Canonical Correlation Analysis~\cite{hotelling1936relations} (CCA) to study the overlap of features in a single layer.
CCA finds the linear combinations of random variables that show maximum correlation with each other.
It is a useful tool to determine if the learned features are overlapping in their representational capacity. 
Li \etal ~\cite{Li2015ConvergentLD} apply correlation analysis to filter activations to show that most of the well-known ConvNet architectures learn similar representations.
Raghu \etal ~\cite{Raghu2017SVCCASV} combine CCA with SVD to perform a correlation analysis of the singular values of activations from various layers.
They show that increasing the depth of a model does not always lead to a corresponding increase of the model's dimensionality, due to several layers learning representations in correlated directions.
We ask an even more elementary question - how correlated are the activations from various filters within a single layer?
In an over-parameterized network like VGG-$16$, which has several convolutional layers with $512$ filters each, it is no surprise that most of the filter activations are highly correlated.
%Thus, it is one of the easily pruned networks with models achieving same performance after dropping over $50$\% of the filters~\cite{Molchanov2016PruningCN, Li2015ConvergentLD}.
As a result, VGG-$16$ has been shown to be easily pruned - more than $50$\% of the filters can be dropped while maintaining the performance of the full network~\cite{Molchanov2016PruningCN, Li2015ConvergentLD}.
Is this also true for significantly smaller convolutional networks, which under-fit the dataset?


\begin{figure}[]
   \includegraphics[width=0.49\linewidth]{figures/repr/Filter_CCA.pdf}
   \includegraphics[width=0.49\linewidth]{figures/repr/Error_Distribution.pdf}
   %%\vspace{-2mm}
   \caption[Correlation Analysis of filters]{Left: Canonical Correlation Analysis of activations from two layers of a ConvNet trained on CIFAR-10. Right: Distribution of change in accuracy when the model is evaluated by dropping one filter at a time.}
   \label{fig:CCA}
   %\vspace{-2mm}
\end{figure}

We will consider a simple network with two convolutional layers of $32$ filters each, and a softmax layer at the end.
Training this model on CIFAR-$10$ for $100$ epochs with an annealed learning rate results in test set accuracy of $58.2$\%, far below the $93.5$\% achieved by VGG-$16$.
%Given the limited capacity of this model every filter is critical to its performance and any correlation in activation would mean limited expressivity of the features and therefore inefficiency in the training.
In the case of VGG-$16$, we might expect that correlation between filters is merely an artifact of the over-parameterization of the model - the dataset simply does not have a dimensionality high enough to require every feature to be orthogonal to every other.
On the other hand, our small network has clearly failed to capture the full feature space of the training data, and thus any correlation between its filters is due to inefficiencies in training, rather than over-parameterization.

%, In order to measure the impact of each filter on the current performance we remove \textit{(zero out),} that filter and run the inference on the test set.
Given a trained model, we can evaluate the contribution of each filter to the model's performance by removing \textit{(zeroing out)} that filter and measuring the drop in accuracy on the test set.
We will call this metric of filter importance the \emph{"greedy Oracle"}.
We perform this evaluation independently for every filter in the model, and plot the distribution of the resulting drops in accuracy in Figure~\ref{fig:CCA} (right).
%We run this independently for all the filters and show the distribution of drop in accuracy due to removing each of the filters in figure~\ref{fig:CCA} (right). This kind of filter importance ranking is known as greedy Oracle.
Most of the second layer filters contribute less than $1\%$ in accuracy and with first layer filters, there is a long tail. 
Some filters are important and contribute over $4\%$ of accuracy but most filters are around $1\%$. 
This implies that even a tiny and under-performing network could be filter pruned without significant performance loss.
The model has not efficiently allocated filters to capture wider representations of necessary features. 
Figure ~\ref{fig:CCA} (left) shows the correlations from linear combinations of the filter activations (CCA) at both the layers.
It is evident that in both the layers there is a significant correlation among filter activations with several of them close to a near perfect correlation of $1$ (\textit{bright yellow spots} $\color{Yellow} \blacksquare$).  The second layer (upper right diagonal) has lot more overlap of features the first layer (lower right).
For a random orthogonal matrix any value above $0.3$ (\textit{lighter than dark blue} $\color{blue} \blacksquare$) is an anomaly.
The activations are even more correlated if the linear combinations are extended to kernel functions~\cite{Hardoon2004CanonicalCA} or singular values~\cite{Raghu2017SVCCASV}.
Regardless, it suffices to say that standard training for convolutional filters does not maximize the representational potential of the network.



 


\section{Our Training Scheme : RePr} \label{sec:training}

We modify the training process by cyclically removing redundant filters, retraining the network, re-initializing the removed filters, and repeating. 
%In order to determine the redundancy of a filter within its layer, we first flatten the $3$D filters into $1$D vectors and correlate them against one another.
We consider each filter ($3$D\textit{ tensor}) as a single unit, and represent it as a long vector - ($f$).
Let $\mathbf{M}$ denote a model with $\mathcal{F}$ filters spread across $\mathbf{L}$ layers. 
Let $\mathcal{\widehat{F}}$ denote a subset of $\mathcal{F}$ filters, such that $\mathbf{M}_\mathcal{F}$ denotes a complete network whereas, $\mathbf{M}_\mathcal{F-\widehat{F}}$ denotes a sub-network without that $\mathcal{\widehat{F}}$ filters.
Our training scheme alternates between training the complete network ($\mathbf{M}_\mathcal{F}$) and the sub-network ($\mathbf{M}_\mathcal{F-\widehat{F}}$).
This introduces two hyper-parameters.
First is the number of iterations to train each of the networks before switching over; let this be $S_1$ for the full network and $S_2$ for the sub-network. 
%If $S_1$ and $S_2$ are equal to $1$, then this scheme becomes stochastic dropout of filters. 
These have to be non-trivial values so that each of the networks learns to improve upon the results of the previous network.
The second hyper-parameter is the total number of times to repeat this alternating scheme; let it be $N$. 
This value has minimal impact beyond certain range and does not require tuning.


The most important part of our algorithm is the metric used to rank the filters. 
Let $\mathcal{R}$ be the metric which associates some numeric value to a filter. 
This could be a norm of the weights or its gradients or our metric - \textit{inter-}filter orthogonality in a layer.
Here we present our algorithm agnostic to the choice of metric. Most sensible choices for filter importance results in an improvement over standard training when applied to our training scheme \textit{(see Ablation Study~\ref{sec:ablation})}.
%We provide a detailed description of our metric and its comparison in the sections below.


Our training scheme operates on a macro-level and is not a weight update rule.
Thus, is not a substitute for SGD or other adaptive methods like Adam~\cite{Kingma2014AdamAM} and RmsProp~\cite{Tieleman2012}. 
Our scheme works with any of the available optimizers and shows improvement across the board. %TODO table reference
However, if using an optimizer that has parameters specific learning rates \textit{(like Adam)}, it is important to re-initialize the learning rates corresponding to the weights
that are part of the pruned filters ($\mathcal{\widehat{F}}$).
Corresponding Batch Normalization~\cite{Ioffe2015BatchNA} parameters ($\gamma \& \beta$) must also be re-initialized.
For this reason, comparisons of our training scheme with standard training are done with a common optimizer.
%TODO add table for optimizer comparison

%We reinitialize the filters ($\mathcal{\widehat{F}}$) to be orthogonal to its value before being dropped and the current value of non-pruned filters ($\mathcal{F-\widehat{F}}$).

Our algorithm is training interposed with \textbf{Re}-initializing and \textbf{Pr}uning - \textbf{RePr} \textit{(pronounced: reaper)}.
We summarize our training scheme in Algorithm~\ref{alg:repr}.

\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    %\For{$n\gets0$ \KwTo $N$}{
    \For{$N$ iterations}{
    
        \For{$S_1$ iterations}{
            Train the full network: $\mathbf{M}_\mathcal{F}$
        }
        
        Compute the metric : $\mathcal{R}(f) \  \forall f\in \mathcal{F}$
        
        Let $\mathcal{\widehat{F}}$ be bottom $p_\%$ of $\mathcal{F}$ using $\mathcal{R}(f)$
        
        \For{$S_2$ iterations}{
            Train the sub-network : $\mathbf{M}_\mathcal{F-\widehat{F}}$
        }
        
        Reinitialize the filters ($\mathcal{\widehat{F}}$) s.t. $\mathcal{\widehat{F}} \perp \mathcal{F}$
        
        {\textit{(and their training specific parameters \\ from BatchNorm and Adam, if applicable)}}
        
    }
    \caption[RePr Training Scheme]{RePr Training Scheme}
    \label{alg:repr}
\end{algorithm}

We use a shallow model to analyze the dynamics of our training scheme and its impact on the train/test accuracy. 
A shallow model will make it feasible to compute the greedy Oracle ranking for each of the filters.
This will allow us to understand the impact of training scheme alone without confounding the results due to the impact of ranking criteria.
We provide results on larger and deeper convolutional networks in Section Results~\ref{sec:results}.

%Consider a three layer vanilla ConvNet, without a skip or dense connections, as shown below.
Consider a $n$ layer vanilla ConvNet, without a skip or dense connections, with X filter each, as shown below:
\begin{equation*}  
    \text{Img} \longmapsto \bigg[ \text{CONV}(X) \rightarrow \text{RELU}  \bigg]^n \longmapsto \text{FC} \longmapsto \text{Softmax}
\end{equation*}

We will represent this architecture as $C^n(X)$. Thus, a $C^3(32)$ has $96$ filters, and when trained with SGD with a learning rate of $0.01$, achieves test accuracy of $73\%$.
Figure~\ref{fig:REPR} shows training plots for accuracy on the training set (left) and test set (right). 
In this example, we use a RePr training scheme with $S_1=20, S_2=10, N=3, p_\%=30$ and the ranking criteria $\mathcal{R}$ as a greedy Oracle.
We exclude a separate validation set of $5$K images from the training set to compute the Oracle ranking.
In the training plot, annotation [A] shows the point at which the filters are first pruned.
Annotation [C] marks the test accuracy of the model at this point. 
The drop in test accuracy at [C] is lower than that of training accuracy at [A], which is not a surprise as most models overfit the training set.
However, the test accuracy at [D] is the same as [C] but at this point, the model only has $70\%$ of the filters. 
This is not a surprising result, as research on filter pruning shows that at lower rates of pruning most if not all of the performance can be recovered~\cite{Molchanov2016PruningCN}.

What is surprising is that test accuracy at [E], which is only a couple of epochs after re-introducing the pruned filters, is significantly higher than point [C].
Both point [C] and point [E] are same capacity networks, and higher accuracy at [E] is not due to the model convergence. 
In the standard training (\textcolor{orange}{orange line}) the test accuracy does not change during this period.
Models that first grow the network and then prune~\cite{Dai2017NeSTAN, Han2015LearningBW}, unfortunately, stopped shy of another phase of growth, which yields improved performance. 
In their defense, this technique defeats the purpose of obtaining a smaller network by pruning. 
However, if we continue RePr training for another two iterations, we see that the point [F], which is still at $70\%$ of the original filters yields accuracy which is comparable
to the point [E] ($100\%$ of the model size.

Another observation we can make from the plots is that training accuracy of RePr model is lower, which signifies some form of regularization on the model. 
This is evident in the Figure~\ref{fig:ablation_percent} (Right), which shows RePr with a large number of iterations ($N=28$). 
While the marginal benefit of higher test accuracy diminishes quickly, the generalization gap between train and test accuracy is reduced significantly.


\section{Our Metric : \textit{inter}-filter orthogonality} \label{sec:metric}

The goals of searching for a metric to rank least important filters are twofold - (1) computing the greedy Oracle is not computationally feasible for large networks, and (2) the greedy Oracle may not be the best criteria. If a filter which captures a unique direction, thus not replaceable by a linear combination of other filters, has a lower contribution to accuracy, the Oracle will drop that filter. On a subsequent re-initialization and training, we may not get back the same set of directions.

The directions captured by the activation pattern expresses the capacity of a deep network~\cite{Raghu2017OnTE}. 
Making orthogonal features will maximize the directions captured and thus expressiveness of the network.
In a densely connected layer, orthogonal weights lead to orthogonal features, even in the presence of ReLU~\cite{Vorontsov2017OnOA}.
However, it is not clear how to compute the orthogonality of a convolutional layer. 

%A parameter of a Convolutional layer is a group of a single kernel, which is only sparsely connected to the previous layer.
A convolutional layer is composed of parameters grouped into spatial kernels and sparsely share the incoming activations.
Should all the parameters in a single convolutional layer be considered while accounting for orthogonality?
The theory that promotes initializing weights to be orthogonal is based on densely connected layers (FC-layers) and popular deep learning libraries follow this guide\footnote{tensorflow:ops/init\_ops.py\#L543 \& pytorch:nn/init.py\#L350} by considering convolutional layer as one giant vector disregarding the sparse connectivity.
A recent attempt to study orthogonality of convolutional filters is described in ~\cite{Xiao2018DynamicalIA} but their motivation is the convergence of very deep networks (10K layers) and not orthogonality of the features.
Our empirical study suggests a strong preference for requiring orthogonality of individual filters in a layer (inter-filter \& intra-layer) rather than individual kernels.


A filter of kernel size $k\times k$ is commonly a $3$D tensor of shape $k \times k \times c$, where $c$ is the number of channels in the incoming activations.
Flatten this tensor to a $1$D vector of size $k*k*c$, and denote it by $f$.
%A convolutional layer usually has more than one filter per layer. 
Let $J_\ell$ denote the number of filters in the layer $\ell$, where $\ell \in \mathbf{L}$, and $\mathbf{L}$ is the number of layers in the ConvNet.
Let $\boldsymbol{W}_\ell$ be a matrix, such that the individual rows are the flattened filters ($f$) of the layer $\ell$.

Let $\boldsymbol{\hat{W}_\ell} = \boldsymbol{W_\ell}/||\boldsymbol{W_\ell}||$ denote the normalized weights.
Then, the measure of Orthogonality for filter $f$ in a layer $\ell$ (denoted by $O_\ell^f$) is computed as shown in the equations below.

% \begin{equation}
%     \boldsymbol{\hat{W}_\ell} = \frac{\boldsymbol{W_\ell}}{||\boldsymbol{W_\ell}||}
% \end{equation}

% \noindent\begin{minipage}{.5\linewidth}
% %\vspace{-1mm}
% \begin{equation}
% %\vspace{-1mm}
%   \boldsymbol{P}_\ell = |\boldsymbol{\hat{W}_\ell} \times \boldsymbol{\hat{W}_\ell}^T - I | 
% \end{equation}
% %\vspace{-0.2mm}
% \end{minipage}%
% \begin{minipage}{.5\linewidth}
% %\vspace{-1mm}
% \begin{equation}
% \label{eqn:ortho}
% %\vspace{-1mm}
%   O^f_\ell = \sum \boldsymbol{P_\ell}[f] / J_\ell
% \end{equation}
% %\vspace{-0.2mm}
% \end{minipage}

%\vspace{-3mm}
\begin{equation}
\boldsymbol{P}_\ell = |\boldsymbol{\hat{W}_\ell} \times \boldsymbol{\hat{W}_\ell}^T - I |
%\vspace{-3mm}
\end{equation}

\begin{equation}
\label{eqn:ortho}
O^f_\ell = \frac{\sum \boldsymbol{P_\ell}[f]}{J_\ell}
%\vspace{-3mm}
\end{equation}

$\boldsymbol{P}_\ell$ is a matrix of size $J_\ell \times J_\ell$ and $\boldsymbol{P}[i]$ denotes $i^{\textit{th}}$ row of $\boldsymbol{P}$.
Off-diagonal elements of a row of $\boldsymbol{P}$ for a filter $f$ denote projection of all the other filters in the same layer with $f$.
The sum of a row is minimum when other filters are orthogonal to this given filter.
We rank the filters least important (thus subject to pruning) if this value is largest among all the filters in the network.
While we compute the metric for a filter over a single layer, the ranking is computed over all the filters in the network.
We do not enforce per layer rank because that would require learning a hyper-parameter $p_\%$ for every layer and some layers are more sensitive than others.
Our method prunes more filters from deeper layers compared to the earlier layers.
This is in accordance with the distribution of contribution of each filter in a given network (Figure~\ref{fig:CCA} right).

Computation of our metric does not require expensive calculations of the inverse of Hessian~\cite{LeCun1989OptimalBD} or the second order derivatives~\cite{Hassibi1992SecondOD} and is feasible for any sized networks. 
The most expensive calculations are $L$ matrix products of size $J_\ell \times J_\ell$, but GPUs are designed for fast matrix-multiplications. 
Still, our method is more expensive than computing norm of the weights or the activations or the Average Percentage of Zeros (APoZ).

Given the choice of Orthogonality of filters, an obvious question would be to ask if adding a soft penalty to the loss function improve this training?
A few researchers  ~\cite{Brock2016NeuralPE, Poole2014AnalyzingNI, Xie2017NearOrthogonalityRI} have reported marginal improvements due to added regularization in the ConvNets used for task-specific models.
We experimented by adding $\lambda * \sum_\ell \boldsymbol{P}_\ell$ to the loss function, but we did not see any improvement. 
Soft regularization penalizes all the filters and changes the loss surface to encourage random orthogonality in the weights without improving expressiveness.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/repr/correlation_combined_reduced.pdf}
    \caption[Correlation of Various Metrics]{Left: Pearson correlation coefficient of various metric values with the accuracy values from greedy Oracle.
   Center: Pearson correlation coefficient of filter ranks using various metric with rank from greedy Oracle \label{fig:corr}}
\end{figure}

\begin{table}[H]
\centering
\rowcolors{1}{White}{Gray}
\begin{tabular}{rl}
                   \multicolumn{2}{c}{\textbf{\shortstack{CIFAR-10}} - $C^3(32)$} \\ \hline
                  %\multicolumn{2}{c}{\textbf{Test Accuracy}} \\ \hline
                  \toprule
                  Standard             & 72.1                                  \\ \hdashline
                  %\multicolumn{2}{c}{RePr} \\ \hdashline
                  \rowcolor[HTML]{EFEFEF} 
                  Random          & 73.4                                  \\
                  Activations          & 74.1                                  \\
                  \rowcolor[HTML]{EFEFEF} 
                  APoZ~\cite{Hu2016NetworkTA}                 & 74.3                                \\
                  Gradients~\cite{LeCun1989OptimalBD}            & 74.3                                  \\
                  \rowcolor[HTML]{EFEFEF} 
                  Taylor~\cite{Molchanov2016PruningCN}               & 74.3                                  \\
                  Hessian~\cite{LeCun1989OptimalBD}              & 74.4                                  \\
                  \rowcolor[HTML]{EFEFEF} 
                  Weights~\cite{Han2015DeepCC}              & 74.6                                  \\
                  Oracle               & 76.0                                  \\
                  \rowcolor[HTML]{EFEFEF} 
                  \textcolor{green}{Ortho}                & \textbf{76.4}                                  \\ \hline
    \end{tabular}
    \caption[Neuron Ranking Metrics]{Test accuracy on CIFAR-10 using standard training and RePr training with various metrics}
  \label{tbl:corr_metrics}
\end{table}

\section{Ablation study} \label{sec:ablation}
%There are some hyper-parameters and function choices in our scheme. 
%We present a short study of the parameters in our scheme.

\begin{figure}[]
\center
    \includegraphics[width=0.8\linewidth]{figures/repr/percentage_pruned_X.pdf}
   \caption[Percentage of filters pruned]{RePr training with various percentage of filters pruned. Shows average test accuracy over 5 epochs starting from epoch 20 for better visibility.}
   \label{fig:ablation_percent}
\end{figure}


\begin{figure}[]
\center
   \includegraphics[width=0.75\linewidth]{figures/repr/marginal_returns_no_y.pdf}
   \caption[Multiple Iterations of RePr]{Marginal returns of multiple iterations of RePR - \textcolor{blue}{Training} and \textcolor{green}{Test} accuracy on CIFAR-10}
   \label{fig:ablation_N_repeat}
\end{figure}



\subsection{Comparison of pruning criteria }
We measure the correlation of our metric with the Oracle to answer the question - how good a substitute is our metric for the filter importance ranking. 
Pearson correlation of our metric, henceforth referred to as Ortho, with the Oracle is $0.38$. 
This is not a strong correlation, however, when we compare this with other known metrics, it is the closest.
Molchanov \etal ~\cite{Molchanov2016PruningCN} report Spearman correlation of their criteria (Taylor) with greedy Oracle at $0.73$. 
We observed similar numbers for Taylor ranking during the early epochs but the correlation diminished significantly as the models converged.
This is due to low gradient value from filters that have converged.
The Taylor metric is a product of the activation and the gradient. 
High gradients correlate with important filters during early phases of learning but when models converge low gradient do not necessarily mean less salient weights.
It could be that the filter has already converged to a useful feature that is not contributing to the overall error of the model or is stuck at a saddle point.
With the norm of activations, the relationship is reversed. 
Thus by multiplying the terms together hope is to achieve a balance. 
But our experiments show that in a fully converged model, low gradients dominate high activations.
Therefore, the Taylor term will have lower values as the models converge and will no longer be correlated with the inefficient filters.
%Most of the well-known pruning criteria assumes low-activity of the neurons. 
While the correlation of the values denotes how well the metric is the substitute for predicting the accuracy, it is more important to measure the correlation of the rank of the filters.
Correlation of the values and the rank may not be the same, and the correlation with the rank is the more meaningful measurement to determine the weaker filters.
Ortho has a correlation of $0.58$ against the Oracle when measured over the rank of the filters. 
Other metrics show very poor correlation using the rank. Figure~\ref{fig:corr} (Left and Center) shows the correlation plot for various metrics with the Oracle.
The table on the right of Figure~\ref{fig:corr} presents the test accuracy on CIFAR-10 of various ranking metrics.
From the table, it is evident that Orthogonality ranking leads to a significant boost of accuracy compared to standard training and other ranking criteria.

\subsection{Percentage of filters pruned }
One of the key factors in our training scheme is the percentage of the filters to prune at each pruning phase ($p_\%$). It behaves like the Dropout parameter, and impacts the training time and generalization ability of the model \textit{(see Figure:~\ref{fig:ablation_percent})}. 
In general the higher the pruned percentage, the better the performance. However, beyond $30\%$, the performances are not significant.
Up to $50\%$, the model seems to recover from the dropping of filters. 
Beyond that, the training is not stable, and sometimes the model fails to converge.
% \begin{figure}[ht]
% \center
%   \includegraphics[width=0.90\linewidth]{figures/repr/marginal_returns.pdf}
%     \caption{\textcolor{blue}{Training} and \textcolor{green}{Test} accuracy over many iterations of RePr}
%     \label{fig:ablation_N}
%     %%\vspace{-2mm}
% \end{figure}

\subsection{Number of RePr iterations }
Our experiments suggest that each repeat of the RePr process has diminishing returns, and therefore should be limited to a single-digit number (see Figure~\ref{fig:ablation_percent} (Right)).
Similar to Dense-Sparse-Dense~\cite{Han2016DSDDT} and Born-Again-Networks~\cite{Furlanello2018BornAN}, we observe that for most networks, two to three iterations is sufficient to achieve the maximum benefit.

\subsection{Optimizer and S1/S2 }
Figure~\ref{fig:ablation_optimizer_s1s2} (left) shows variance in improvement when using different optimizers. 
Our model works well with most well-known optimizers. Adam and Momentum perform better than SGD due to their added stability in training. 
We experimented with various values of $S1$ and $S2$, and there is not much difference if either of them is large enough for the model to converge temporarily. 
\begin{figure}[]
   \includegraphics[width=1.0\linewidth]{figures/repr/ablation_optimizer_s1s2.pdf}
   %%\vspace{-2mm}
   \caption[Optimizers on RePr]{Left: Impact of using various optimizers on RePr training scheme. Right: Results from using different S1/S2 values. For clarity, these experiments only shows results with $S1=S2$}
   \label{fig:ablation_optimizer_s1s2}
   %\vspace{-2mm}
\end{figure}

\subsection{Learning Rate Schedules}
%Optimal accuracy on SGD is not achieved at a fixed learning rate. 
SGD with a fixed learning rate does not typically produce optimal model performance.
%It is well-known that gradually annealing the learning rate finds better test accuracy.
Instead, gradually annealing the learning rate over the course of training is known to produce models with higher test accuracy.
State-of-the-art results on ResNet, DenseNet, Inception were all reported with a predetermined learning rate schedule.
%However, finding the right schedule is a hyper-parameter, and what works for some models may not work for others. 
However, the selection of the exact learning rate schedule is itself a hyperparameter, one which needs to be specifically tuned for each model.
%Cyclical learning rates\footnote{Smith, Leslie N.. “Cyclical Learning Rates for Training Neural Networks.” WACV (2017)} reduce the need to tune the frequency and rate for LR schedule.
Cyclical learning rates~\cite{Smith2017CyclicalLR} can provide stronger performance without exhaustive tuning of a precise learning rate schedule.
Figure~\ref{fig:LR} shows the comparison of our training technique when applied in conjunction with fixed schedule learning rate scheme and cyclical learning rate. 
Our training scheme is not impacted by using these schemes, and improvements over standard training is still apparent.

\begin{figure}[]
\hspace*{-4.0mm}
\center
   \includegraphics[width=0.47\linewidth]{figures/repr/RePr_LR_Staircase.pdf}
   \hspace{-2.0mm}
   \includegraphics[width=0.47\linewidth]{figures/repr/RePr_LR_Cycle.pdf}
   %\vspace{-3mm}
   \caption[Learning Rate Schedule]{Test accuracy of a three layer ConvNet with 32 filters each over $100$ epochs using \textcolor{orange}{standard scheme} and \textcolor{blue}{our method - RePr} on CIFAR-$10$. The shaded regions denote periods when only part of the network is trained for RePr.
   Left: Fixed Learning Rate schedule of $0.1$, $0.01$ and $0.001$.Right: Cyclic Learning Rate with periodicity of $50$ Epochs, and amplitude of 0.005 and starting LR of 0.001.}
   \label{fig:LR}
   %%\vspace{1mm}
\end{figure}

\subsection{Impact of Dropout}
%Dropout is commonly not used in Convolutional Neural Networks.
Dropout, while commonly applied in Multilayer Perceptrons, is typically not used for ConvNets.
%However, our model could be considered as Dropout with non-random criteria. 
Our technique can be viewed as a type of non-random Dropout, specifically applicable to ConvNets.
%It should be noted that while Dropout is applied to individual weights and across all iterations, our technique is applied to filters and only at select stages.
Unlike standard Dropout, out method acts on entire filters, rather than individual weights, and is applied only during select stages of training, rather than in every training step.
%Dropout prevents overfitting by co-adaptation, this is necessary with over-parameterized models.
Dropout prevents overfitting by encouraging co-adaptation of weights.
This is effective in the case of over-parameterized models, but in compact or shallow models, Dropout may needlessly reduce already limited model capacity.
%With compact and shallow models it might not be the case, and dropout effectively reduces the model capacity.

Figure~\ref{fig:dropout} shows the performance of Standard Training and our proposed method (RePr) with and without Dropout on a three-layer convolutional neural network with $32$ filters each. 
Dropout was applied with a probability of $0.5$.
%Overall test accuracy is lower with Dropout, which is primarily due to Dropout reducing the model capacity by half. 
We observe that the inclusion of Dropout lowers the final test accuracy, due to the effective reduction of the model's capacity by half.
%However, our training scheme (RePr) applied with our proposed metric - \textit{inter}-filter orthogonality - still provides improves performance over the standard training. 
Our method produces improved performance with or without the addition of standard Dropout, demonstrating that its effects are distinct from the benefits of Dropout. 

\begin{figure}[]
%\hspace*{-2mm}
\center
   \includegraphics[width=0.48\linewidth]{figures/repr/with_dropout.pdf}
   \includegraphics[width=0.48\linewidth]{figures/repr/without_dropout.pdf}
   %\vspace{-3mm}
   \caption[Impact of Dropout]{Test accuracy of a three layer ConvNet with 32 filters each over $100$ epochs using \textcolor{red}{standard scheme}, \textcolor{blue}{RePr with Oracle} and \textcolor{green}{RePr with Ortho} on CIFAR-$10$. Left: With Dropout of $0.5$. Right: No Dropout}
   \label{fig:dropout}
   %%\vspace{1mm}
\end{figure}

\textbf{Orthogonal Loss - OL}
Adding Orthogonality of filters (equation 1) as a regularization term as a part of the optimization loss does not significantly impact the performance of the model.
Thus, the loss function will be -
\begin{equation*}
    \mathcal{L} = \text{Cross entropy} + \lambda * |\boldsymbol{\hat{W}_\ell} \times \boldsymbol{\hat{W}_\ell}^T - I |
\end{equation*}
where, $\lambda$ is a hyper-parameter which balances both the cost terms. We experimented with various values of $\lambda$. Table~\ref{tbl:ortho_loss} report the results with this loss term for the $\lambda = 0.01$, for which the validation accuracy was the highest. OL refers to addition of this loss term.

\begin{table}[H]
\center
\begin{tabular}{ccccc}
\toprule
          $C^3(32)$ & \multicolumn{1}{c}{\textbf{Std}} & \multicolumn{1}{c}{\textbf{Std+OL}} & \multicolumn{1}{c}{\textbf{RePr}} & \multicolumn{1}{c}{\textbf{RePr+OL}} \\ \hline
CIFAR-10  & 72.1                                                     & 72.8                                                    & 76.4                                                      & {\color[HTML]{3166FF} 76.7}                                                         \\
CIFAR-100 & 47.2                                                     & 48.3                                                    & 58.2                                                      & {\color[HTML]{3166FF} 58.6}                                                        \\ \bottomrule
\end{tabular}
%%\vspace{-2mm}
\caption[Orthogonality Loss]{Comparison of addition of Orthogonality loss to Standard Training and RePr}
\label{tbl:ortho_loss}
%%\vspace{-2mm}
\end{table}


\begin{figure}[]
   \includegraphics[width=1.0\linewidth]{figures/repr/orthogonality_distillation.pdf}
   %%\vspace{-2mm}
   \caption[Comparison with Knowledge Distillation]{Comparison of orthogonality of filters (Ortho-sum - eq 2) in standard training and RePr training with and without Knowledge Distillation. \textbf{Lower value} signifies less overlapping filters.
   Dashed vertical lines denotes filter dropping.}
   \label{fig:orthodistill}
   %%\vspace{-2mm}
\end{figure}

\begin{table}[]
\center
\begin{tabular}{
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c }
\toprule
          $C^3(32)$ & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{Std}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{KD}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{RePr}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{KD+RePr}} \\ \hline
CIFAR-10  & 72.1                                                     & 74.8                                                    & 76.4                                                      & \textbf{83.1}                                                         \\
CIFAR-100 & 47.2                                                     & 56.5                                                    & 58.2                                                      & \textbf{64.1}                                                        \\ \bottomrule
\end{tabular}
%%\vspace{-2mm}
\caption[Comparison of Knowledge Distillation with RePr]{Comparison of Knowledge Distillation with RePr.}
\label{tbl:distillation}
%%\vspace{-2mm}
\end{table}

\section{Orthogonality and Distillation} \label{lbl:distillation}
Our method, RePr and Knowledge Distillation (KD) are both techniques to improve performance of compact models.
RePr reduces the overlap of filter representations and KD distills the information from a larger network. 
We present a brief comparison of the techniques and show that they can be combined to achieve even better performance.

RePr repetitively drops the filters with most overlap in the directions of the weights using the \textit{inter}-filter orthogonality, as shown in the equation~\ref{eqn:ortho}. 
Therefore, we expect this value to gradually reduce over time during training.
Figure~\ref{fig:orthodistill} (left) shows the sum of this value over the entire network with three training schemes.
%\textcolor{red}{Base Model} is the standard training which does not involve our RePr scheme.
We show RePr with two different filter ranking criteria - \textcolor{green}{Ortho} and \textcolor{blue}{Oracle}. 
It is not surprising that RePr training scheme with Ortho ranking has lowest Ortho sum but it is surprising that RePr training with Oracle ranking also reduces the filter overlap, compared to the standard training.
Once the model starts to converge, the least important filters based on Oracle ranking are the ones with the most overlap.
And dropping these filters leads to better test accuracy \textit{(table on the right of  Figure~\ref{fig:corr}).}
Does this improvement come from the same source as the that due to Knowledge Distillation?
Knowledge Distillation (KD) is a well-proven methodology to train compact models. 
Using soft logits from the teacher and the ground truth signal the model converges to better optima compared to standard training. 
If we apply KD to the same three experiments (see Figure~\ref{fig:orthodistill}, right), we see that all the models have significantly larger Ortho sum. Even the RePr (Ortho) model struggles to lower the sum as the model is strongly guided to converge to a specific solution.
This suggests that this improvement due to KD is not due to reducing filter overlap. Therefore, a model which uses both the techniques should benefit by even better generalization.
Indeed, that is the case as the combined model has significantly better performance than either of the individual models, as shown in  Table~\ref{tbl:distillation}. 

\section{Results} \label{sec:results}

\subsection{Image Classification}
We present the performance of our training scheme, RePr, with our ranking criteria, \textit{inter}-filter orthogonality, Ortho, on different ConvNets~\cite{Simonyan2014VeryDC, He2016DeepRL, Szegedy2015GoingDW, Szegedy2016RethinkingTI, Huang2017DenselyCC}.
For all the results provided RePr parameters are: $S_1=20$, $S_2=10$, $p_\%=30$, and with three iterations, $N=3$.

\begin{figure}[]
\center
   \includegraphics[width=1.0\linewidth]{figures/repr/ablation_layers.pdf}
   %%\vspace{-2mm}
   \caption[Impact of model depth]{Accuracy improvement using RePr over standard training on Vanilla ConvNets across many layered networks [$C^n(32)$]}
   \label{fig:ablation_layers}
   %%\vspace{-2mm}
\end{figure}

\begin{table}[]
\center
\begin{tabular}{cccccc}
\toprule
\multicolumn{6}{c}{\textbf{ResNet-20 on CIFAR-10}}                                                                                                                                                                                                                                                                                                     \\ \toprule
\multicolumn{2}{c}{Baseline}                                                                                 & \multicolumn{4}{c}{Various Training Schemes}                                                                                                                                                                                                    \\
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Original\\ ~\cite{He2016DeepRL}\end{tabular}}}        & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Our \\ Impl\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}DSD\\ ~\cite{Han2016DSDDT}\end{tabular}}}             & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}BAN\\ ~\cite{Furlanello2018BornAN}\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}RePr\\ Weights\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}RePr \\ Ortho\end{tabular}}} \\ \hline
\multicolumn{1}{c}{8.7} & \multicolumn{1}{c|}{8.4}                                                         & \multicolumn{1}{c}{7.8} & \multicolumn{1}{c|}{8.2}          & 7.7                                                                                 & \textcolor{blue}{6.9}                     \\
\bottomrule
\end{tabular}
    \caption[RePr vs DSD vs BAN]{Comparison of test error from using various techniques. }
   \label{tbl:resnet20}
   %%\vspace{-2mm}
\end{table}

We compare our training scheme with other similar schemes like BAN and DSD in table~\ref{tbl:resnet20}.
All three schemes were trained for three iterations \ie N=3. 
All models were trained for 150 epochs with similar learning rate schedule and initialization.
DSD and RePr (Weights) perform roughly the same function - sparsifying the model guided by magnitude, with the difference that DSD acts on individual weights, while RePr (Weights) acts on entire filters.
Thus, we observe similar performance between these techniques.
%The primary difference between DSD and RePr (Weights) is that DSD applies sparsity at the level of individual weights, while RePr does so the level of blocks (filters), and thus results are not too different. 
RePr (Ortho) outperforms the other techniques and is significantly cheaper to train compared to BAN, which requires N full training cycles.

Compared to modern architectures, vanilla ConvNets show significantly more inefficiency in the allocation of their feature representations. 
Thus, we find larger improvements from our method when applied to vanilla ConvNets, as compared to modern architectures. 
Table~\ref{tbl:cifar} shows test errors on CIFAR 10 \& 100. 
Vanilla CNNs with 32 filters each have high error compared to DenseNet or ResNet but their inference time is significantly faster. 
RePr training improves the relative accuracy of vanilla CNNs by $~8\%$ on CIFAR-10 and $~25\%$ on CIFAR-100.
The performance of baseline DenseNet and ResNet models is still better than vanilla CNNs trained with RePr, but these models incur more than twice the inference cost.
For comparison, we also consider a reduced DenseNet model with only $5$ layers, which has similar inference time to the 3-layer vanilla ConvNet. 
%Compare DenseNet with $5$ layers (not part of the original paper), which has similar inference time with 3 layer vanilla CNN.
This model has many fewer parameters (by a factor of $11\times$) than the vanilla ConvNet, leading to significantly higher error rates, but we choose to equalize inference time rather than parameter count, due to the importance of inference time in many practical applications.
%This model is much weaker than the vanilla CNN, which could be attributed to reduced capacity ($11\times$ fewer parameters), but for many practical applications, low inference time is more important than the model capacity.
Figure~\ref{fig:ablation_layers} shows more results on vanilla CNNs with varying depth.
Vanilla CNNs start to overfit the data, as most filters converge to similar representation.
Our training scheme forces them to be different which reduces the overfitting (Figure~\ref{fig:ablation_percent} - right).
This is evident in the larger test error of 18-layer vanilla CNN with CIFAR-10 compared to 3-layer CNN.
With RePr training, $18$ layer model shows lower test error.

RePr is also able to improve the performance of ResNet and shallow DenseNet.
This improvement is larger on CIFAR-100, which is a $100$ class classification and thus is a harder task and requires more specialized filters.
Similarly, our training scheme shows bigger relative improvement on ImageNet, a $1000$ way classification problem.
Table~\ref{tbl:imagenet} presents top-1 test error on ImageNet~\cite{Russakovsky2015ImageNetLS} of various ConvNets trained using standard training and with RePr.
RePr was applied three times (N=3), and the table shows errors after each round.
We have attempted to replicate the results of the known models as closely as possible with suggested hyper-parameters and are within $\pm 1\%$ of the reported results.
More details of the training and hyper-parameters are provided in the supplementary material.
Each subsequent RePr leads to improved performance with significantly diminishing returns.
Improvement is more distinct in architectures which do not have skip connections, like Inception v1 and VGG and have lower baseline performance.

\begin{figure}[H]
   \includegraphics[width=0.999\linewidth]{figures/repr/filter_CCA_3_second_rebuttal.pdf}
%   \vspace{-2mm}
   \caption[Comparison of Filter Overlap]{Canonical Correlation Analysis of activations from the $1^\text{st}$ and $2^\text{nd}$ layer of a C$2^(32)$ ConvNet trained on CIFAR-10.}
   \label{fig:CCA_after}
%   \vspace{-4mm}
\end{figure}

Figure ~\ref{fig:CCA_after}) provides a side-by-side comparison of correlation of activations on the layers of a two-layer ConvNet when trained with standard training scheme and when trained with RePr. While RePr is not able to remove all the high correlation filters, it significantly reduces the overlap of feature representation. For the first layer the differences are less pronounced due to less overlapping filters in the standard training.



\begin{table}[ht]
\center
\begin{tabular}{ccccccc}
\toprule
                  &                                                                            & \multicolumn{1}{c}{}                                                                        & \multicolumn{2}{c}{\textbf{CIFAR-10}}                                                                & \multicolumn{2}{c}{\textbf{CIFAR-100}}     \\ \hline
Layers & \begin{tabular}[c]{@{}c@{}}Params\\ {\scriptsize($\times 000$)}\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Inf. Time\\ {\scriptsize(relative)}\end{tabular}} & Std               & \multicolumn{1}{c}{RePr}                                       & Std & RePr               \\ \midrule
\multicolumn{7}{c}{Vanilla CNN {[}32 filters / layer{]}}                                                                                                                                                                                                                                                                                                   \\ \hline
\rowcolor[HTML]{EFEFEF} 
3                 & 20                                                                         & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}1.0}                                             & 27.9                       & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{3166FF} 23.6}} & 52.8         & {\color[HTML]{3166FF} 41.8} \\
8                 & 66                                                                         & \multicolumn{1}{c}{1.7}                                                                     & 26.8                       & \multicolumn{1}{c}{{\color[HTML]{3166FF} 19.5}}                         & 50.9         & {\color[HTML]{3166FF} 36.8} \\
\rowcolor[HTML]{EFEFEF} 
13                & 113                                                                        & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}2.5}                                             & 26.6                       & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{3166FF} 20.6}} & 51.0         & {\color[HTML]{3166FF} 37.9} \\
18                & 159                                                                        & \multicolumn{1}{c}{3.3}                                                                     & 28.2                       & \multicolumn{1}{c}{{\color[HTML]{3166FF} 22.5}}                         & 51.9         & {\color[HTML]{3166FF} 39.5} \\ \hline
\multicolumn{7}{c}{DenseNet {[}k=12{]}}                                                                                                                                                                                                                                                                                                            \\ \hline
\rowcolor[HTML]{EFEFEF} 
5                 & 1.7                                                                        & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.9}                                             & 39.4                       & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{3166FF} 36.2}} & 43.5         & {\color[HTML]{3166FF} 40.9} \\
40                & 1016                                                                       & \multicolumn{1}{c}{8.0}                                                                     & 6.8                        & \multicolumn{1}{c}{{\color[HTML]{3166FF} 6.2}}                          & 26.4         & {\color[HTML]{3166FF} 25.2} \\
\rowcolor[HTML]{EFEFEF} 
100               & 6968                                                                       & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}43.9}                                            & {\color[HTML]{3166FF} 5.3} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} 5.6}}  & 22.2         & {\color[HTML]{000000} 22.1} \\ \hline
\multicolumn{7}{c}{ResNet}                                                                                                                                                                                                                                                                                                                         \\ \hline
\rowcolor[HTML]{EFEFEF} 
20                & 269                                                                        & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}1.7}                                             & 8.4                        & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{3166FF} 6.9}}  & 32.6         & {\color[HTML]{3166FF} 31.1} \\
32                & 464                                                                        & \multicolumn{1}{c}{2.2}                                                                     & 7.4                        & \multicolumn{1}{c}{{\color[HTML]{3166FF} 6.1}}                          & 31.4         & {\color[HTML]{3166FF} 30.1} \\
\rowcolor[HTML]{EFEFEF} 
110               & 1727                                                                       & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}7.1}                                             & 6.3                        & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{3166FF} 5.4}}  & 27.5         & {\color[HTML]{3166FF} 26.4} \\
182               & 2894                                                                       & \multicolumn{1}{c}{11.7}                                                                    & 5.6                        & \multicolumn{1}{c}{{\color[HTML]{3166FF} 5.1}}                          & 26.0         & {\color[HTML]{3166FF} 25.3} \\ \bottomrule
\end{tabular}
\caption[Results on CIFAR-10/100]{Comparison of test error on Cifar-10 \& Cifar-100 of various ConvNets using Standard training vs RePr Training. Inf. Time shows the inference times for a single pass. All time measurements are relative to Vanilla CNN with three layers. Parameter count does not include the last fully-connected layer.}
   \label{tbl:cifar}
\end{table}

\begin{table}[H]
\center
\begin{tabular}{cccccc}
\toprule
\multicolumn{6}{c}{\textbf{ImageNet}} \\
\midrule
      & Standard                             & \multicolumn{3}{c}{RePr Training}             &      Relative                       \\ \cline{3-5}
Model & Training & N=1 & N=2 & N=3 & Change \\ \midrule
\rowcolor[HTML]{EFEFEF} 
ResNet-18      & 30.41                       & 28.68        & 27.87        & {\color[HTML]{3166FF} 27.31} & -11.35                      \\
ResNet-34      & 27.50                       & 26.49        & 26.06        & {\color[HTML]{3166FF} 25.80} & -6.59                       \\
\rowcolor[HTML]{EFEFEF} 
ResNet-50      & 23.67                       & 22.79        & 22.51        & {\color[HTML]{3166FF} 22.37} & -5.81                       \\
ResNet-101     & 22.40                       & 21.70        & 21.51        & {\color[HTML]{3166FF} 21.40} & -4.67                       \\
\rowcolor[HTML]{EFEFEF} 
ResNet-152     & 21.51                       & 20.99        & 20.79        & {\color[HTML]{3166FF} 20.71} & -3.86                       \\
VGG-16         & 31.30                       & 27.76        & 26.45        & {\color[HTML]{3166FF} 25.50} & -22.75                      \\
\rowcolor[HTML]{EFEFEF} 
Inception v1   & 31.11                       & 29.41        & 28.47        & {\color[HTML]{3166FF} 28.01} & -11.07                      \\
Inception v2   & 27.60                       & 27.15        & 26.95        & {\color[HTML]{3166FF} 26.80} & -2.99                       \\ \bottomrule
\end{tabular}
   \caption[Results on ImageNet]{Comparison of test error (Top-1) on ImageNet with different models at various stages of RePr. N=1, N=2, N=3 are results after each round of RePr.}
   \label{tbl:imagenet}
\end{table}

Our model improves upon other computer vision tasks that use similar ConvNets. We present a small sample of results from visual question answering and object detection tasks.
Both these tasks involve using ConvNets to extract features, and RePr improves their baseline results.

\subsection{Visual Question Answering}

In the domain of visual question answering (VQA), a model is provided with an image and question (as text) about that image, and must produce an answer to that question.
Most of the models that solve this problem use standard ConvNets to extract image features and an LSTM network to extract text features.
These features are then fed to a third model which learns to select the correct answer as a classification problem. State-of-the-art models use an attention layer and intricate mapping between features. We experimented with a more standard model where image features and language features are fed to a Multi-layer Perceptron with a softmax layer at the end that does $1000$-way classification over candidate answers.
Table~\ref{tbl:vqa} provides accuracy on VQAv1 using VQA-LSTM-CNN model~\cite{Antol2015VQAVQ}.
Results are reported for Open-Ended questions, which is a harder task compared to multiple-choice questions.
We extract image features from Inception-v1, trained using standard training and with RePr (Ortho) training, and then feed these image features and the language embeddings (GloVe vectors) from the question, to a two layer fully connected network. 
Thus, the only difference between the two reported results~\ref{tbl:vqa} is the training methodology of Inception-v1.
Figure ~\ref{fig:vqa_quality} shows qualitative results on VQA. Even in cases where the top-1 choice does not change RePR is able to improve the confidence of the prediction.


\begin{table}[H]
\begin{tabular}{llll}
\multicolumn{2}{c}{\includegraphics[width=0.48\linewidth]{figures/repr/frisbee.jpg}}                                                                                 & \multicolumn{2}{c}{\includegraphics[width=0.48\linewidth]{figures/repr/dog.jpg}}                                                                   \\
\multicolumn{2}{c}{\large What are they playing with?}                                                                                 & \multicolumn{2}{c}{\large What animal is in the picture?}                                                                   \\ \midrule
Standard & \begin{tabular}[c]{@{}l@{}}0.4 - Soccer ball\\ 0.3 - Frisbee\\ 0.2 - Soccer\end{tabular}   & Standard & \begin{tabular}[c]{@{}l@{}}0.6 - Donkey\\ 0.3 - Dog\\ 0.1 - No\end{tabular}  \\ \midrule
RePr     & \begin{tabular}[c]{@{}l@{}}{\color[HTML]{3166FF} 0.7 - Frisbee}\\ 0.2 - Soccer ball\\ 0.1 - Baseball\end{tabular} & RePr     & \begin{tabular}[c]{@{}l@{}}{\color[HTML]{3166FF} 0.5 - Dog}\\ 0.3 - Donkey\\ 0.1 - Cow\end{tabular} \\ \midrule
\end{tabular}
\caption[Qualitative Results on VQA]{Sample of differences in confidence of selected answers between Standard Trainin and RePr training}
\label{fig:vqa_quality}
\end{table}


\begin{table}[H]
\center
\rowcolors{1}{White}{Gray}
\begin{tabular}{ccccc}
        \toprule
         & \textbf{All} & \textbf{Yes/No} & \textbf{Other} & \textbf{Number} \\ \midrule
Standard & 60.3    &    81.4    &  47.6     &  37.2      \\
RePr (Ortho)    & {\color[HTML]{3166FF} 64.6}    &  {\color[HTML]{3166FF} 83.4}      &  {\color[HTML]{3166FF} 54.5}     &  37.2     \\ \bottomrule
\end{tabular}
\caption[Results on VQA]{Comparison of Standard Training and RePr on VQA using VQA-LSTM-CNN model}
\label{tbl:vqa}
\end{table}

\subsection{Object Detection}

For object detection, we experimented with Faster R-CNN using ResNet $50$ and $101$ pretrained on ImageNet.
We experimented with both Feature Pyramid Network and baseline RPN with \textit{c4} conv layer.
We use the model structure from Tensorpack~\cite{wu2016tensorpack}, which is able to reproduce the reported mAP scores.
The model was trained on 'trainval35k + minival' split of COCO dataset (2014). 
Mean Average Precision (mAP) is calculated at ten IoU thresholds from $0.5$ to $0.95$. 
mAP for the boxes obtained with standard training and RePr training is shown in the table~\ref{tbl:rpn}.

\begin{table}[H]
\center
\rowcolors{1}{White}{Gray}
\begin{tabular}{ccccc}
        \toprule
         & \multicolumn{2}{l}{\textbf{ResNet-50}} & \multicolumn{2}{l}{\textbf{ResNet-101}} \\
         & RPN           & FPN           & RPN           & FPN           \\ \midrule
Standard  &      38.1  & 38.2      &  40.7     & 41.7\\
RePr (Ortho) &   {\color[HTML]{3166FF} 41.1}  & {\color[HTML]{3166FF} 42.3}     &   {\color[HTML]{3166FF} 43.5}     & {\color[HTML]{3166FF} 44.5}       \\ \bottomrule
\end{tabular}
\caption[Results on Object Detection]{mAP scores with Standard and RePr (Ortho) training for object detection with ResNet the ConvNet (RPN on C4)}
\label{tbl:rpn}
\end{table}

\section{Conclusion} \label{sec:conclusion}
We have introduced RePr, a training paradigm which cyclically drops and relearns some percentage of the least expressive filters.
After dropping these filters, the pruned sub-model is able to recapture the lost features using the remaining parameters, allowing a more robust and efficient allocation of model capacity once the filters are reintroduced.
%Our model parameterizes the frequency and timing of filter dropping, and demonstrate significant improvement from proper settings for these parameters.
We show that a reduced model needs training before re-introducing the filters, and careful selection of this training duration leads to substantial gains. 
We also demonstrate that this process can be repeated with diminishing returns.
%By choosing the frequency and the timing of dropping the filters, we demonstrate significant improvement in generalization ability over standard training.
%Unlike previous approaches, our model takes a parametric approach to the frequency and the timing of the drop. 
%This has a significant impact on performance, as the sub-model is able to learn the dropped representation leading to better allocation of reintroduced filters.

Motivated by prior research which highlights inefficiencies in the feature representations learned by convolutional neural networks, we further introduce a novel \textit{inter}-filter orthogonality metric for ranking filter importance for the purpose of RePr training, and demonstrate that this metric outperforms established ranking metrics.
%We show that instead of using stochastic dropping or well-known filter pruning criteria, our proposed \textit{inter}-filter orthogonality ranking yields the best result. 
%Our ranking is motivated by the research in the inefficiency of feature representation in convolutional neural networks.
%Our metric is motivated by prior research which highlights inefficiencies in the feature representations learned by convolutional neural networks.
%Our training technique significantly improves the performance of smaller and simpler ConvNets and is complementary to Knowledge Distillation.
Our training method is able to significantly improve performance in under-parameterized networks by ensuring the efficient use of limited capacity, and the performance gains are complementary to knowledge distillation.
Even in the case of complex, over-parameterized network architectures, our method is able to improve performance across a variety of tasks.
