Advancements in Deep Neural Networks has made a tremendous impact in various tasks. 
However, these methods have their limitations. 
Most of the CNN and RNN based models are computationally expensive and require special hardware to operate at a reasonable rate.
More importantly, these methods are easily fooled by small changes in the input. 
These limitations are a hindrance to the widespread use of these models and prevent use in critical applications.
In this dissertation, I discuss these issues in details and propose solutions to overcome both these issues.

Yet another issue with a lot of research in deep learning is their limited use-case beyond typical examples.
I explore applications of these novel methods to image compression, paraphrase generation, and disease diagnosis.
By changing the existing structure or the training process we are able to apply deep learning methods to these tasks.
Our methods yield results that are significantly better than standard methods without paying significantly higher in computational cost.
